{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1JefRs175DCikD-6UPxFN1S5egalAO7JA",
      "authorship_tag": "ABX9TyNUZyZNIkomcO2Fb0us6GNa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCWKwHpt7N-M",
        "outputId": "1ad606ac-91c0-485f-eeb6-5b02e1046ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/ISR/datasets/song_lyrics.csv.zip\n",
            "  inflating: /content/song_lyrics.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/drive/MyDrive/ISR/datasets/song_lyrics.csv.zip' -d '/content/'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path = '/content/drive/MyDrive/ISR/datasets/preprocessed_datasets'"
      ],
      "metadata": {
        "id": "S1qCU7x2OaF8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "chunk_size = 100000\n",
        "\n",
        "# Read CSV file in chunks\n",
        "chunks = pd.read_csv('song_lyrics.csv', usecols=['title', 'artist', 'lyrics', 'year', 'language'], chunksize=chunk_size)\n",
        "\n",
        "# List to store filtered chunks\n",
        "filtered_chunks = []\n",
        "\n",
        "# Process each chunk\n",
        "for i, chunk in enumerate(chunks):\n",
        "    # Filter rows where the language is English ('en')\n",
        "    english_rows = chunk[chunk['language'] == 'en']\n",
        "\n",
        "    # Append filtered chunk to the list\n",
        "    filtered_chunks.append(english_rows)\n",
        "\n",
        "# Concatenate filtered chunks into a single DataFrame\n",
        "filtered_df = pd.concat(filtered_chunks)\n",
        "\n",
        "# # Write the filtered DataFrame to a new CSV file\n",
        "# filtered_df.to_csv('song_lyrics_english.csv', index=False)\n",
        "csv_filename = f'{drive_path}/song_lyrics_english.csv'\n",
        "filtered_df.to_csv(csv_filename, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ds-Yct_D9_vz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count\n",
        "#datasize = 5140000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyq_KNoh-Yls",
        "outputId": "32999624-2ca2-4242-bcd1-da8dad7cd513"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "514"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywJ4RoYXAR9B",
        "outputId": "92a116f6-7eda-46e3-9fb2-ec0857ae7fe0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3374198"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import re\n",
        "# import string\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# # Download stopwords corpus\n",
        "# nltk.download('stopwords')\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "# def preprocess_text(text):\n",
        "#   #1 lowercase\n",
        "#   text = text.lower()\n",
        "\n",
        "#   #2 replace special characters\n",
        "#   text = re.sub('\\n+', ' ', text)\n",
        "#   text = re.sub('[^\\w\\s]', ' ', text)\n",
        "#   text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "#   return text\n",
        "\n",
        "\n",
        "# def tokenize_text(text):\n",
        "\n",
        "#     # Tokenize text\n",
        "#     tokens = nltk.word_tokenize(text)\n",
        "\n",
        "#     # Remove stop words\n",
        "#     tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "#     # # Join tokens back into a single string\n",
        "#     # tokenized_text = ' '.join(tokens)\n",
        "\n",
        "#     return tokens\n"
      ],
      "metadata": {
        "id": "MYpwFP26DMSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import csv\n",
        "# from collections import Counter\n",
        "# import re\n",
        "\n",
        "# # Preprocess and tokenize each row\n",
        "# #'title', 'artist', 'lyrics', 'year'\n",
        "# def preprocess_row(row):\n",
        "#     artist = row['artist']\n",
        "#     title = row['title']\n",
        "#     year = row['year']\n",
        "#     #preprocessed_lyrics = preprocess_text(row['lyrics'])\n",
        "\n",
        "#     preprocessed_lyrics = row['lyrics'].lower()\n",
        "#     preprocessed_lyrics = preprocessed_lyrics.replace('\\n',' ')\n",
        "#     # Tokenization\n",
        "#     #tokens = tokenize_text(preprocessed_title + ' ' + preprocessed_lyrics)\n",
        "\n",
        "#     return {\n",
        "#         'artist': artist,\n",
        "#         'title': title,\n",
        "#         'year': year,\n",
        "#         'lyrics': preprocessed_lyrics\n",
        "#     }\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxgGAbhnCvdk",
        "outputId": "213a3eba-9945-4763-d8dd-e8b37e51d241"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# song_documents = []\n",
        "# with open('song_lyrics_english.csv', newline='', encoding='utf-8') as csvfile:\n",
        "#     reader = csv.DictReader(csvfile)\n",
        "#     for row in reader:\n",
        "#         preprocessed_row = preprocess_row(row)\n",
        "#         song_documents.append(preprocessed_row)\n",
        "#         song_count_token.update(preprocessed_row['tokens'])"
      ],
      "metadata": {
        "id": "uPExeFaZKX92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Write the preprocessed data to a new CSV file\n",
        "# with open('preprocessed_songs.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "#     fieldnames = ['artist', 'title', 'year', 'lyrics']\n",
        "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "#     writer.writeheader()\n",
        "#     for row in song_documents:\n",
        "#         writer.writerow(row)"
      ],
      "metadata": {
        "id": "ZjKuxKcrFvT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import math\n",
        "\n",
        "# # Sort song_documents by year\n",
        "# sorted_song_documents = sorted(song_documents, key=lambda x: x['year'])\n",
        "\n",
        "# # Define the number of files to split the sorted data into\n",
        "# num_files = 20\n",
        "\n",
        "# # Calculate the number of records per file\n",
        "# records_per_file = math.ceil(len(sorted_song_documents) / num_files)\n",
        "\n",
        "# # Split the sorted data into chunks for each file\n",
        "# chunks = [sorted_song_documents[i:i+records_per_file] for i in range(0, len(sorted_song_documents), records_per_file)]\n",
        "\n",
        "# # Write each chunk to a separate CSV file\n",
        "# for i, chunk in enumerate(chunks):\n",
        "#     filename = f'preprocessed_songs_{i+1}.csv'  # File name format: preprocessed_songs_1.csv, preprocessed_songs_2.csv, etc.\n",
        "#     with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "#         writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "#         writer.writeheader()\n",
        "#         for row in chunk:\n",
        "#             writer.writerow(row)\n"
      ],
      "metadata": {
        "id": "z6oibZOKIkY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UTOKOztsLa-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cmIEdG87LbWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import math\n",
        "\n",
        "# Preprocess and tokenize each row\n",
        "def preprocess_row(row):\n",
        "    artist = row['artist']\n",
        "    title = row['title']\n",
        "    year = row['year']\n",
        "    preprocessed_lyrics = row['lyrics'].lower().replace('\\n', ' ')\n",
        "    return {\n",
        "        'artist': artist,\n",
        "        'title': title,\n",
        "        'year': year,\n",
        "        'lyrics': preprocessed_lyrics\n",
        "    }\n",
        "\n",
        "# Define the chunk size\n",
        "chunk_size = 100000\n",
        "\n",
        "# Open input and output CSV files\n",
        "\n",
        "with open('song_lyrics_english.csv', newline='', encoding='utf-8') as infile:\n",
        "    reader = csv.DictReader(infile)\n",
        "    for i, row in enumerate(reader):\n",
        "        # Preprocess the row\n",
        "        preprocessed_row = preprocess_row(row)\n",
        "\n",
        "        # Write each chunk to a separate CSV file\n",
        "        if i % chunk_size == 0:\n",
        "            chunk_index = i // chunk_size\n",
        "            # filename = f'preprocessed_songs_chunk_{chunk_index}.csv'\n",
        "            # with open(filename, 'w', newline='', encoding='utf-8') as outfile:\n",
        "\n",
        "            filename = f'{drive_path}/preprocessed_songs_chunk_{chunk_index}.csv'\n",
        "            with open(filename, 'w', newline='', encoding='utf-8') as outfile:\n",
        "                writer = csv.DictWriter(outfile, fieldnames=preprocessed_row.keys())\n",
        "                writer.writeheader()\n",
        "                writer.writerow(preprocessed_row)\n",
        "        else:\n",
        "            with open(filename, 'a', newline='', encoding='utf-8') as outfile:  # Open the file in 'append' mode\n",
        "                writer = csv.DictWriter(outfile, fieldnames=preprocessed_row.keys())\n",
        "                writer.writerow(preprocessed_row)\n",
        "\n",
        "\n",
        "    # Write the remaining data to a CSV file\n",
        "    filename = f'{drive_path}/preprocessed_songs_chunk_{chunk_index+1}.csv'\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as outfile:\n",
        "        writer = csv.DictWriter(outfile, fieldnames=preprocessed_row.keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerow(preprocessed_row)\n"
      ],
      "metadata": {
        "id": "0zsfet3NLb3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "7_PzKVo5OQpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uNnpyJ7i0OjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiment analysis #store artist, title, sentiments"
      ],
      "metadata": {
        "id": "u-7pRA0W0O3n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}