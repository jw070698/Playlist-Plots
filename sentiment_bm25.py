# -*- coding: utf-8 -*-
"""sentiment-bm25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/161toSc_Tk28HCR0CwnKEg4Fj3FNkdZIZ

### Text Sentiment Analysis for New Data
"""

import pandas as pd
new_song = pd.read_csv('Songs.csv')
new_song

import re
def remove_non_alpha(text):
    return re.sub(r'[^a-zA-Z0-9\s]', '', text)

new_song_lyrics = new_song['Lyric'].astype(str).apply(lambda x: x.lower())
new_song_lyrics = new_song_lyrics.str.replace('\n',' ')
print(new_song_lyrics)

!pip install nrclex

import nltk
nltk.download('punkt')

from nrclex import NRCLex
for i in range(10):
    new_emotion = NRCLex(new_song_lyrics[i])
    print(new_song_lyrics[i], '\n', new_emotion.top_emotions, new_emotion.raw_emotion_scores, '\n')

new_emotion1 = NRCLex(new_song_lyrics[0])
print(new_emotion1.raw_emotion_scores)
new_emotion2 = NRCLex(new_song_lyrics[1])
print(new_emotion2.raw_emotion_scores)

# lyrics
new_results_lyrics = []
new_results_lyrics_score = []
for i in range(len(new_song_lyrics)):
    emotion = NRCLex(new_song_lyrics[i])
    raw_score = emotion.raw_emotion_scores
    if len(raw_score) > 1:
        mean_score = sum(raw_score.values()) / len(raw_score)
        std_dev = (sum((x - mean_score) ** 2 for x in raw_score.values()) / len(raw_score)) ** 0.5
    else:
        std_dev = 1  # Set std_dev to a non-zero value to avoid division by zero

    if std_dev == 0:
        normalized_score = {emotion: 0 for emotion, score in raw_score.items()}
    else:
        normalized_score = {emotion: (score - mean_score) / std_dev for emotion, score in raw_score.items()}

    if normalized_score:  # Check if the dictionary is not empty
        max_score = max(normalized_score.values())
        min_score = min(normalized_score.values())
    else:
        max_score, min_score = 0, 0

    if max_score != min_score:
        normalized_score = {emotion: (score - min_score) / (max_score - min_score) for emotion, score in normalized_score.items()}
    else:
        normalized_score = {emotion: 0 for emotion, score in raw_score.items()}

    formatted_score = {emotion: format(score, '.4f') for emotion, score in normalized_score.items()}


    new_results_lyrics.append(new_song.loc[i]['Lyric'])
    new_results_lyrics_score.append(formatted_score)

# lyrics to dataframe
id_df = pd.DataFrame(new_results_lyrics)
artist = new_song['Artist']
Title = new_song['Title']
Year = new_song['Year']
score_df = pd.DataFrame(new_results_lyrics_score)
merged_df = pd.concat([artist,Title,Year,id_df, score_df], axis=1)
new_results_lyrics_df = pd.DataFrame(merged_df)

new_results_lyrics_df.rename(columns={new_results_lyrics_df.columns[3]: 'Lyrics'}, inplace=True)

new_results_lyrics_df

"""## Text Sentiment Analysis

- Using nrclex
- https://pypi.org/project/NRCLex/
"""

import pandas as pd
book_info = pd.read_csv('Best_Books_Ever.csv')
music_info = pd.read_csv('Songs.csv')

import re
def remove_non_alpha(text):
    return re.sub(r'[^a-zA-Z0-9\s]', '', text)

book_description = book_info['description'].astype(str).apply(lambda x: x.lower())
book_description = book_description.str.replace('\n',' ')
print(book_description)

music_info

music_lyrics = music_info['Lyric'].astype(str).apply(lambda x: x.lower())
music_lyrics = music_lyrics.str.replace('\n',' ')
print(music_lyrics)

!pip install nrclex

import nltk
nltk.download('punkt')

from nrclex import NRCLex
for i in range(10):
    emotion = NRCLex(book_description[i])
    print(book_description[i], '\n', emotion.top_emotions, emotion.raw_emotion_scores, '\n')

for i in range(10):
    emotion = NRCLex(music_lyrics[i])
    print(music_lyrics[i], '\n', emotion.top_emotions, emotion.raw_emotion_scores, '\n')

emotion1 = NRCLex(music_lyrics[0])
print(emotion1.raw_emotion_scores)
emotion2 = NRCLex(music_lyrics[1])
print(emotion2.raw_emotion_scores)

# Check if it has devided by zero
'''
emotion = NRCLex(music_lyrics[36])
raw_score = emotion.raw_emotion_scores
mean_score = sum(raw_score.values()) / len(raw_score)
std_dev = (sum((x - mean_score) ** 2 for x in raw_score.values()) / len(raw_score)) ** 0.5
print(raw_score)
normalized_score = {emotion: (score - mean_score) / std_dev for emotion, score in raw_score.items()}
max_score = max(normalized_score.values())
min_score = min(normalized_score.values())
normalized_score = {emotion: (score - min_score) / (max_score - min_score) for emotion, score in normalized_score.items()}
formatted_score = {emotion: format(score, '.4f') for emotion, score in normalized_score.items()}
print(formatted_score)'''

# lyrics
results_lyrics = []
results_lyrics_score = []
for i in range(len(music_lyrics)):
    emotion = NRCLex(music_lyrics[i])
    raw_score = emotion.raw_emotion_scores
    if len(raw_score) > 1:
        mean_score = sum(raw_score.values()) / len(raw_score)
        std_dev = (sum((x - mean_score) ** 2 for x in raw_score.values()) / len(raw_score)) ** 0.5
    else:
        std_dev = 1  # Set std_dev to a non-zero value to avoid division by zero

    if std_dev == 0:
        normalized_score = {emotion: 0 for emotion, score in raw_score.items()}
    else:
        normalized_score = {emotion: (score - mean_score) / std_dev for emotion, score in raw_score.items()}

    if normalized_score:  # Check if the dictionary is not empty
        max_score = max(normalized_score.values())
        min_score = min(normalized_score.values())
    else:
        max_score, min_score = 0, 0

    if max_score != min_score:
        normalized_score = {emotion: (score - min_score) / (max_score - min_score) for emotion, score in normalized_score.items()}
    else:
        normalized_score = {emotion: 0 for emotion, score in raw_score.items()}

    formatted_score = {emotion: format(score, '.4f') for emotion, score in normalized_score.items()}


    results_lyrics.append(music_info.loc[i]['Lyric'])
    results_lyrics_score.append(formatted_score)

# lyrics to dataframe
id_df = pd.DataFrame(results_lyrics)
score_df = pd.DataFrame(results_lyrics_score)
merged_df = pd.concat([id_df, score_df], axis=1)

# lyrics to csv
results_lyrics_df = pd.DataFrame(merged_df)
results_lyrics_df.to_csv('NormalizationLyricsScore.csv', index=False)

# book descriptions
results_descriptions = []
results_descriptions_score = []
for i in range(len(book_description)):
    emotion = NRCLex(book_description[i])
    raw_score = emotion.raw_emotion_scores
    if len(raw_score) > 1:
        std_dev = (sum((x - mean_score) ** 2 for x in raw_score.values()) / len(raw_score)) ** 0.5
        mean_score = sum(raw_score.values()) / len(raw_score)
    else:
        std_dev = 1  # Set std_dev to a non-zero value to avoid division by zero

    if std_dev == 0:
        normalized_score = {emotion: 0 for emotion, score in raw_score.items()}
    else:
        normalized_score = {emotion: (score - mean_score) / std_dev for emotion, score in raw_score.items()}

    if normalized_score:  # Check if the dictionary is not empty
        max_score = max(normalized_score.values())
        min_score = min(normalized_score.values())
    else:
        max_score, min_score = 0, 0

    if max_score != min_score:
        normalized_score = {emotion: (score - min_score) / (max_score - min_score) for emotion, score in normalized_score.items()}
    else:
        normalized_score = {emotion: 0 for emotion, score in raw_score.items()}

    formatted_score = {emotion: format(score, '.4f') for emotion, score in normalized_score.items()}


    results_descriptions.append(book_info.loc[i]['title'])
    results_descriptions_score.append(formatted_score)

# book descriptions to dataframe
genres = pd.DataFrame(book_info["genres"])
id_df2 = pd.DataFrame(results_descriptions)
score_df2 = pd.DataFrame(results_descriptions_score)
merged_df2 = pd.concat([id_df2,genres, score_df2], axis=1)

# book descriptions to csv
results_description_df = pd.DataFrame(merged_df2)
results_description_df.to_csv('NormalizationDescriptionScore.csv', index=False)
results_description_df.rename(columns={results_description_df.columns[0]: 'book'}, inplace=True)

results_description_df.head(20)

results_description_df = results_description_df.drop(['negative', 'positive'], axis=1)
new_results_lyrics_df = new_results_lyrics_df.drop(['negative', 'positive'], axis=1)

def fetch_lyrics(book_name, sample_description, new_results_lyrics_df):

    book_row = sample_description[sample_description['book'] == book_name]
    if book_row.empty:
        return "Book not found in the dataset."


    emotions = ['anticipation', 'disgust', 'joy', 'sadness', 'surprise', 'trust', 'anger', 'fear']
    book_row[emotions] = book_row[emotions].apply(pd.to_numeric, errors='coerce').fillna(0)


    return new_results_lyrics_df

book_name='Gone with the Wind'
author_name='Margaret Mitchell'

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def calculate_cosine_similarity(book_vector, song_vectors):
    book_vector = np.nan_to_num(book_vector, nan=0)
    song_vectors = np.nan_to_num(song_vectors, nan=0)
    similarities = cosine_similarity(book_vector.reshape(1, -1), song_vectors)
    return similarities.flatten()

def fetch_lyrics_and_calculate_similarity(book_name, results_description_df, results_lyrics_df):
    lyrics_with_max_emotion = fetch_lyrics(book_name, results_description_df, results_lyrics_df)

    if isinstance(lyrics_with_max_emotion, str):
        return lyrics_with_max_emotion

    book_row = results_description_df[results_description_df['book'] == book_name]
    emotions = ['anticipation', 'disgust', 'joy', 'sadness', 'surprise', 'trust', 'anger', 'fear']
    book_emotions = book_row[emotions].values.flatten().astype(float)

    song_emotions = lyrics_with_max_emotion[emotions].values.astype(float)

    similarities = calculate_cosine_similarity(book_emotions, song_emotions)

    lyrics_with_max_emotion['cos_similarity'] = similarities

    most_similar_songs = lyrics_with_max_emotion.sort_values(by='cos_similarity', ascending=False).head(500)
    #most_similar_songs = most_similar_songs.sort_values(by='Year', ascending=False)
    print(f"Input Book: {book_name}")
    print(f"Genres: {book_row['genres'].values[0]}")
    return most_similar_songs

book_name_input = book_name
output_most_similar_songs = fetch_lyrics_and_calculate_similarity(book_name_input, results_description_df, new_results_lyrics_df)
print(output_most_similar_songs)

type(output_most_similar_songs)

"""# BM25 analysis"""

import pandas as pd
import re
import string
import nltk
from nltk.corpus import stopwords

import nltk
nltk.download('punkt')

# Download stopwords corpus
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))


def preprocess_text(text):
  #1 lowercase
  text = text.lower()

  #2 replace special characters
  text = re.sub('\n+', ' ', text)
  text = re.sub('[^\w\s]', ' ', text)
  text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

  return text


def tokenize_text(text):

    # Tokenize text
    tokens = nltk.word_tokenize(text)

    # Remove stop words
    tokens = [token for token in tokens if token not in stop_words]

    # # Join tokens back into a single string
    # tokenized_text = ' '.join(tokens)

    return tokens

import csv
from collections import Counter
import re

song_documents = []
song_count_token = Counter()

with open('Songs.csv', newline='', encoding='utf-8') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        #sanitize all cols
        artist = row['Artist']
        prep_artist = preprocess_text(row['Artist'])
        title = row['Title']
        preprocessed_title = preprocess_text(row['Title'])
        album = preprocess_text(row['Album'])
        year = preprocess_text(row['Year'])
        date = preprocess_text(row['Date'])
        lyric = preprocess_text(row['Lyric'])

        # Tokenization
        tokens = tokenize_text(prep_artist + ' ' + preprocessed_title + ' ' + album + ' ' + lyric)

        # tokens = [word for word in tokens if all(c.isalpha() for c in word)]
        # tokens = [word for word in tokens]
        song_count_token.update(tokens)

        song_documents.append({'document_id': (title, artist), 'tokens': tokens, 'lyric': lyric, 'album': album, 'title': title, 'artist': artist})

"""# Book summary and metadata tokenization and preprocessing"""

import csv
from collections import Counter
import re

book_documents = []
book_count_token = Counter()

with open('Best_Books_Ever.csv', newline='', encoding='utf-8') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        #sanitize all cols
        author = row['author']
        title = row['title']
        preprocessed_author = preprocess_text(row['author'])
        preprocessed_title = preprocess_text(row['title'])
        preprocessed_description = preprocess_text(row['description'])
        genre = row['genres']
        location = row['setting']

        # Tokenization
        tokens = tokenize_text(preprocessed_title + ' ' + preprocessed_description)
        tokens.append(genre)
        tokens.append(location)

        # tokens = [word for word in tokens if all(c.isalpha() for c in word)]
        # tokens = [word for word in tokens]
        book_count_token.update(tokens)

        book_documents.append({'document_id': (title, author), 'tokens': tokens, 'description': description, 'genre': genre})

"""# BM matching between one book and songs from previous results

"""

# (artist, title) pair from sentiment analysis result

import pandas as pd

new_result_song_documents = []
for index, row in output_most_similar_songs.iterrows():

    artist = row['Artist']
    title = row['Title']

    matching_documents = [doc for doc in song_documents if doc['artist'] == artist and doc['title'] == title]

    new_result_song_documents.extend(matching_documents)

# Print the result document list
print(new_result_song_documents)

len(new_result_song_documents)

#Ranking based on BM25
!pip install rank_bm25

from rank_bm25 import BM25Okapi

corpus = [doc['tokens'] for doc in new_result_song_documents]

bm25 = BM25Okapi(corpus)

import requests

def get_book_summary(book_title, author):
    url = f"https://www.googleapis.com/books/v1/volumes?q=intitle:{book_title}+inauthor:{author}&key=AIzaSyC7l7kUyLRQHu0flXHakLAlnPDhDXQaHu4"
    response = requests.get(url)

    if response.status_code == 200:
        data = response.json()
        if 'items' in data and len(data['items']) > 0:
            item = data['items'][0]
            summary = item['volumeInfo'].get('description', 'Summary not available.')
            return summary
        else:
            return "Book not found."
    else:
        return "Failed to retrieve book summary."

matching_document = next((doc for doc in book_documents if doc['document_id'] == (book_name, author_name)), None)

# Print the matching document
if matching_document:
    print("Matching Document:", matching_document)
else:
    print("No matching document found.")

#book data from dataset
book_1 = matching_document
book_1_tokens = book_1['tokens']

#fix later for case where book not in our dataset
#book data from api
book_2_description = get_book_summary(book_1['document_id'][0], book_1['document_id'][1])
print(book_2_description)


if book_2_description != 'Book not found.' and book_2_description != 'Failed to retrieve book summary.':
  book_2_description = preprocess_text(book_2_description)
  book_2_tokens = tokenize_text(book_2_description)
else:
  book_2_tokens = []

#book tokens as query
tokenized_query = book_1_tokens + book_2_tokens
#print(tokenized_query)
doc_scores = bm25.get_scores(tokenized_query)

#Printing top 10 document ids, for songs which contains query tokens in their lyrics, ranking based on BM25 Okapi
result = bm25.get_top_n(tokenized_query, corpus, n=50)
print(book_1['document_id'][0])
for res in result:
  for doc in new_result_song_documents:
    if doc['tokens'] == res:
      print(doc['document_id'])
      break